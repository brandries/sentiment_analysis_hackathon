{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('train_data.csv')\n",
    "targets = pd.read_csv('train_reponse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CuAG9zG2VDdd4hPGMip2Xg</td>\n",
       "      <td>Deeeelicious!\\n\\nFirst, the upstairs is a pret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oipDuz40GWRdhFI_ck0hmQ</td>\n",
       "      <td>Just had a baby!\\n\\n...about a month ago. Had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k7IdlhwtZ2evNJOkaKmPoQ</td>\n",
       "      <td>Since I was staying in the hotel we had a $25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xg1qeM_nYE0r0PYBZQedzg</td>\n",
       "      <td>I can't believe that I haven't reviewed this s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u3eX3oMz3hC2KfX3mdKBlA</td>\n",
       "      <td>Mmmm so much choice and it's all completely aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                                               text\n",
       "0  CuAG9zG2VDdd4hPGMip2Xg  Deeeelicious!\\n\\nFirst, the upstairs is a pret...\n",
       "1  oipDuz40GWRdhFI_ck0hmQ  Just had a baby!\\n\\n...about a month ago. Had ...\n",
       "2  k7IdlhwtZ2evNJOkaKmPoQ  Since I was staying in the hotel we had a $25 ...\n",
       "3  xg1qeM_nYE0r0PYBZQedzg  I can't believe that I haven't reviewed this s...\n",
       "4  u3eX3oMz3hC2KfX3mdKBlA  Mmmm so much choice and it's all completely aw..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CuAG9zG2VDdd4hPGMip2Xg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oipDuz40GWRdhFI_ck0hmQ</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k7IdlhwtZ2evNJOkaKmPoQ</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xg1qeM_nYE0r0PYBZQedzg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u3eX3oMz3hC2KfX3mdKBlA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id  stars\n",
       "0  CuAG9zG2VDdd4hPGMip2Xg      4\n",
       "1  oipDuz40GWRdhFI_ck0hmQ      5\n",
       "2  k7IdlhwtZ2evNJOkaKmPoQ      3\n",
       "3  xg1qeM_nYE0r0PYBZQedzg      5\n",
       "4  u3eX3oMz3hC2KfX3mdKBlA      5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feat.merge(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df['text'])\n",
    "y = df['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deeeelicious!\\n\\nFirst, the upstairs is a pret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just had a baby!\\n\\n...about a month ago. Had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Since I was staying in the hotel we had a $25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe that I haven't reviewed this s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mmmm so much choice and it's all completely aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tried to go today at lunchtime:  but didn't re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>To me, this is the BEST Sushi place in town an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i miss this place so much, it's insanity. \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Love San Tan Village. It's conveniently locate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>no hanky panky here.. the idea of these places...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Throughout this review I've debated between 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I've stayed in just about every hotel in Vegas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Food is amazing and so is the ambiance - elega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Never again...went in for a part and have them...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I believe the cronuts are the best product the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>They have a great selection but the prices are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>If I could give this please more starts I easi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I am officially 5 lbs heavier than I was 2 hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ok Yelpers... this is the epitome of  value, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I love this Barnes and Noble. It is always nea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Weera Thai is a wonderful Thai restaurant loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>We have stayed at this hotel once but we will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>It's fun to say, isn't it? Go ahead, it just r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>So many hours clocked in here I can't begin to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The service was great! Horchata was awesome an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I like the set up. Staff is friendly. Kind of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ordered for delivery and never received my ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Quick service. The employees actually seem hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Stopped in here just for some drinks and apps ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I've only just now read the reviews here for t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499970</th>\n",
       "      <td>I got to get my dream sandwich the other day, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499971</th>\n",
       "      <td>Love to bowl here! Great clean lanes and well ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499972</th>\n",
       "      <td>Horrible place to go.. It's small and the loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499973</th>\n",
       "      <td>steak &amp; eggs!\\n\\nsteak and eggs for $3.99? i'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499974</th>\n",
       "      <td>Good fish &amp; chips, good service and nice locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499975</th>\n",
       "      <td>I have gone to this location when it was Angel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499976</th>\n",
       "      <td>They have these in the South Bay where I grew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499977</th>\n",
       "      <td>My friend and I went in here on a Sunday after...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499978</th>\n",
       "      <td>This show was a blast! They have stunning visu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499979</th>\n",
       "      <td>The frozen hot chocolate is PERFECT during sum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499980</th>\n",
       "      <td>lovely spot to grab a late night dinner for tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499981</th>\n",
       "      <td>I love Jupiter's pizza. Lots of different exce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499982</th>\n",
       "      <td>I feel compelled to write an additional review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499983</th>\n",
       "      <td>This Fry's makes me feel like I am at the Ritz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499984</th>\n",
       "      <td>It's been 3 days since I ate here and I'm stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499985</th>\n",
       "      <td>Pretty bad. Wait staff nice and friendly but t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499986</th>\n",
       "      <td>On my last day of my boys bachelor party we ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499987</th>\n",
       "      <td>Wow a lot of bad experiences for people at thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499988</th>\n",
       "      <td>Tucked away in a sort of strip mall (next to R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499989</th>\n",
       "      <td>The service here is terrible.  The place was e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499990</th>\n",
       "      <td>Saying this hotel is below average is kind. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499991</th>\n",
       "      <td>DO NOT STAY HERE!!!!!\\n\\nHorrible experience l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499992</th>\n",
       "      <td>Over priced and bad service. Last time I ate t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499993</th>\n",
       "      <td>Popped in to check out the happy hour specials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499994</th>\n",
       "      <td>I like to come during happy hour. They have a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>I went here a short time ago with a social net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>Fresh and yummy.  I nice change from Pho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>I love there thin crust pizza... They would wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>So faer this has been the best shop I have bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>I would stay away from this Store. They do not...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       Deeeelicious!\\n\\nFirst, the upstairs is a pret...\n",
       "1       Just had a baby!\\n\\n...about a month ago. Had ...\n",
       "2       Since I was staying in the hotel we had a $25 ...\n",
       "3       I can't believe that I haven't reviewed this s...\n",
       "4       Mmmm so much choice and it's all completely aw...\n",
       "5       tried to go today at lunchtime:  but didn't re...\n",
       "6       To me, this is the BEST Sushi place in town an...\n",
       "7       i miss this place so much, it's insanity. \\n\\n...\n",
       "8       Love San Tan Village. It's conveniently locate...\n",
       "9       no hanky panky here.. the idea of these places...\n",
       "10      Throughout this review I've debated between 3 ...\n",
       "11      I've stayed in just about every hotel in Vegas...\n",
       "12      Food is amazing and so is the ambiance - elega...\n",
       "13      Never again...went in for a part and have them...\n",
       "14      I believe the cronuts are the best product the...\n",
       "15      They have a great selection but the prices are...\n",
       "16      If I could give this please more starts I easi...\n",
       "17      I am officially 5 lbs heavier than I was 2 hou...\n",
       "18      Ok Yelpers... this is the epitome of  value, a...\n",
       "19      I love this Barnes and Noble. It is always nea...\n",
       "20      Weera Thai is a wonderful Thai restaurant loca...\n",
       "21      We have stayed at this hotel once but we will ...\n",
       "22      It's fun to say, isn't it? Go ahead, it just r...\n",
       "23      So many hours clocked in here I can't begin to...\n",
       "24      The service was great! Horchata was awesome an...\n",
       "25      I like the set up. Staff is friendly. Kind of ...\n",
       "26      Ordered for delivery and never received my ord...\n",
       "27      Quick service. The employees actually seem hap...\n",
       "28      Stopped in here just for some drinks and apps ...\n",
       "29      I've only just now read the reviews here for t...\n",
       "...                                                   ...\n",
       "499970  I got to get my dream sandwich the other day, ...\n",
       "499971  Love to bowl here! Great clean lanes and well ...\n",
       "499972  Horrible place to go.. It's small and the loca...\n",
       "499973  steak & eggs!\\n\\nsteak and eggs for $3.99? i'm...\n",
       "499974  Good fish & chips, good service and nice locat...\n",
       "499975  I have gone to this location when it was Angel...\n",
       "499976  They have these in the South Bay where I grew ...\n",
       "499977  My friend and I went in here on a Sunday after...\n",
       "499978  This show was a blast! They have stunning visu...\n",
       "499979  The frozen hot chocolate is PERFECT during sum...\n",
       "499980  lovely spot to grab a late night dinner for tw...\n",
       "499981  I love Jupiter's pizza. Lots of different exce...\n",
       "499982  I feel compelled to write an additional review...\n",
       "499983  This Fry's makes me feel like I am at the Ritz...\n",
       "499984  It's been 3 days since I ate here and I'm stil...\n",
       "499985  Pretty bad. Wait staff nice and friendly but t...\n",
       "499986  On my last day of my boys bachelor party we ca...\n",
       "499987  Wow a lot of bad experiences for people at thi...\n",
       "499988  Tucked away in a sort of strip mall (next to R...\n",
       "499989  The service here is terrible.  The place was e...\n",
       "499990  Saying this hotel is below average is kind. Th...\n",
       "499991  DO NOT STAY HERE!!!!!\\n\\nHorrible experience l...\n",
       "499992  Over priced and bad service. Last time I ate t...\n",
       "499993  Popped in to check out the happy hour specials...\n",
       "499994  I like to come during happy hour. They have a ...\n",
       "499995  I went here a short time ago with a social net...\n",
       "499996           Fresh and yummy.  I nice change from Pho\n",
       "499997  I love there thin crust pizza... They would wi...\n",
       "499998  So faer this has been the best shop I have bee...\n",
       "499999  I would stay away from this Store. They do not...\n",
       "\n",
       "[500000 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tw.tokenize(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deeeelicious',\n",
       " '!',\n",
       " 'First',\n",
       " ',',\n",
       " 'the',\n",
       " 'upstairs',\n",
       " 'is',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'area',\n",
       " 'to',\n",
       " 'sit',\n",
       " 'and',\n",
       " 'wait',\n",
       " 'for',\n",
       " 'a',\n",
       " 'table.',\n",
       " 'When',\n",
       " 'we',\n",
       " 'arrived',\n",
       " 'we',\n",
       " 'realized',\n",
       " 'we',\n",
       " 'forgot',\n",
       " 'to',\n",
       " 'make',\n",
       " 'reservations',\n",
       " '(',\n",
       " 'sigh',\n",
       " ')',\n",
       " 'and',\n",
       " 'were',\n",
       " 'forced',\n",
       " 'to',\n",
       " 'wait',\n",
       " 'around',\n",
       " 'for',\n",
       " 'an',\n",
       " 'hour.',\n",
       " 'But',\n",
       " 'luckily',\n",
       " 'they',\n",
       " 'have',\n",
       " 'a',\n",
       " 'bar',\n",
       " 'and',\n",
       " 'seating',\n",
       " 'area',\n",
       " 'upstairs',\n",
       " 'where',\n",
       " 'you',\n",
       " 'can',\n",
       " 'order',\n",
       " 'drinks',\n",
       " 'and',\n",
       " 'appetizers',\n",
       " 'while',\n",
       " 'you',\n",
       " 'wait',\n",
       " '!',\n",
       " 'I',\n",
       " 'had',\n",
       " 'wine',\n",
       " ',',\n",
       " 'my',\n",
       " 'boyfriend',\n",
       " 'had',\n",
       " 'beer',\n",
       " 'and',\n",
       " 'we',\n",
       " 'of',\n",
       " 'course',\n",
       " 'ordered',\n",
       " 'some',\n",
       " 'cheese',\n",
       " 'curds.',\n",
       " 'They',\n",
       " 'are',\n",
       " 'so',\n",
       " 'good',\n",
       " ',',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'best',\n",
       " 'in',\n",
       " 'Madison',\n",
       " '!',\n",
       " 'Perfectly',\n",
       " 'seasoned',\n",
       " 'and',\n",
       " 'still',\n",
       " 'gooey',\n",
       " 'inside.',\n",
       " 'When',\n",
       " 'we',\n",
       " 'were',\n",
       " 'finally',\n",
       " 'seated',\n",
       " ',',\n",
       " 'I',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'as',\n",
       " 'hungry',\n",
       " 'as',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'I',\n",
       " \"'d\",\n",
       " 'be',\n",
       " '(',\n",
       " 'probably',\n",
       " 'from',\n",
       " 'eating',\n",
       " 'almost',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'curds',\n",
       " ')',\n",
       " 'so',\n",
       " 'I',\n",
       " 'ordered',\n",
       " 'a',\n",
       " 'soup',\n",
       " 'and',\n",
       " 'salad',\n",
       " 'combo.',\n",
       " 'You',\n",
       " 'get',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'food',\n",
       " 'for',\n",
       " 'a',\n",
       " 'cheap',\n",
       " 'price',\n",
       " '(',\n",
       " '$',\n",
       " '6.99',\n",
       " ')',\n",
       " '.',\n",
       " 'The',\n",
       " 'soup',\n",
       " 'was',\n",
       " 'a',\n",
       " 'creamy',\n",
       " 'tomato',\n",
       " 'soup',\n",
       " 'and',\n",
       " 'the',\n",
       " 'salad',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'tossed',\n",
       " 'salad',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'feta.',\n",
       " 'Both',\n",
       " 'were',\n",
       " 'yummy',\n",
       " ',',\n",
       " 'I',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'the',\n",
       " 'soup',\n",
       " 'though',\n",
       " 'and',\n",
       " 'I',\n",
       " 'could',\n",
       " 'have',\n",
       " 'eaten',\n",
       " 'an',\n",
       " 'entire',\n",
       " 'bowl',\n",
       " 'of',\n",
       " 'it.',\n",
       " 'My',\n",
       " 'boyfriend',\n",
       " 'had',\n",
       " 'the',\n",
       " 'Craftsman',\n",
       " 'burger',\n",
       " 'and',\n",
       " 'really',\n",
       " 'enjoyed',\n",
       " 'it.',\n",
       " 'Service',\n",
       " 'was',\n",
       " 'great',\n",
       " ',',\n",
       " 'too.',\n",
       " 'We',\n",
       " 'will',\n",
       " 'be',\n",
       " 'back',\n",
       " ',',\n",
       " 'next',\n",
       " 'time',\n",
       " 'with',\n",
       " 'reservations',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string, punctuation):\n",
    "    tokens = TreebankWordTokenizer().tokenize(string)\n",
    "    tokens = [x for x in tokens if x not in punctuation]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punc = list(punctuation)\n",
    "punc = [x for x in punc if x not in ['!', '?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e0dbfc458e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'no_punc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpunc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3179\u001b[0m         \u001b[0;31m# handle ufuncs and lambdas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3181\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3183\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-3d992b482be2>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(string, punctuation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X['no_punc'] = X['text'].apply(preprocessing, args=[punc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect.fit_transform(X['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)\n",
    "y.columns = ['stars_{}'.format(x) for x in y.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375000, 1000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "opt = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=128, activation='relu', input_dim=len(vect.get_feature_names())))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(5))\n",
    "\n",
    "model.compile(opt, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 130,277\n",
      "Trainable params: 130,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 337500 samples, validate on 37500 samples\n",
      "Epoch 1/10\n",
      "337500/337500 [==============================] - 10s 31us/step - loss: 3.7846 - val_loss: 4.3545\n",
      "Epoch 2/10\n",
      "337500/337500 [==============================] - 6s 19us/step - loss: 5.9698 - val_loss: 7.4325\n",
      "Epoch 3/10\n",
      "337500/337500 [==============================] - 7s 21us/step - loss: 6.8889 - val_loss: 6.7086\n",
      "Epoch 4/10\n",
      "337500/337500 [==============================] - 7s 20us/step - loss: 6.9654 - val_loss: 7.0616\n",
      "Epoch 5/10\n",
      "337500/337500 [==============================] - 10s 30us/step - loss: 7.2712 - val_loss: 7.3693\n",
      "Epoch 6/10\n",
      "337500/337500 [==============================] - 6s 18us/step - loss: 7.5190 - val_loss: 7.6490\n",
      "Epoch 7/10\n",
      "337500/337500 [==============================] - 11s 33us/step - loss: 7.7417 - val_loss: 7.8859\n",
      "Epoch 8/10\n",
      "337500/337500 [==============================] - 9s 27us/step - loss: 7.9335 - val_loss: 8.0599\n",
      "Epoch 9/10\n",
      "337500/337500 [==============================] - 6s 19us/step - loss: 8.0909 - val_loss: 8.1859\n",
      "Epoch 10/10\n",
      "337500/337500 [==============================] - 7s 22us/step - loss: 8.2195 - val_loss: 8.3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3eaa0baf60>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=500, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125000/125000 [==============================] - 7s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.28729291168213"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
